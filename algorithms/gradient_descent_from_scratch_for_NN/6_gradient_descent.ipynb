{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h2 style=\"color:green\" align=\"center\">Implement Gradient Descent For Neural Network (or Logistic Regression)</h2>"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h4 style=\"color:blue\">Predicting if a person would buy life insurnace based on his age using logistic regression</h4>"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Above is a binary logistic regression problem as there are only two possible outcomes (i.e. if person buys insurance or he/she doesn't). "
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(\"insurance_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Split train and test set**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['age','affordibility']],df.bought_insurance,test_size=0.2, random_state=25)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Preprocessing: Scale the data so that both age and affordibility are in same scaling range**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled['age'] = X_train_scaled['age'] / 100\n",
    "\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled['age'] = X_test_scaled['age'] / 100"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"nn.png\" height=800 width=800/>"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, input_shape=(2,), activation='sigmoid', kernel_initializer='ones', bias_initializer='zeros')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=5000)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Evaluate the model on test set**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.evaluate(X_test_scaled,y_test)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.predict(X_test_scaled)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_test"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Now get the value of weights and bias from the model**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "coef, intercept = model.get_weights()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "coef, intercept"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**This means w1=5.060867, w2=1.4086502, bias =-2.9137027**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sigmoid(x):\n",
    "        import math\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "sigmoid(18)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_test"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Instead of model.predict, write our own prediction function that uses w1,w2 and bias**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def prediction_function(age, affordibility):\n",
    "    weighted_sum = coef[0]*age + coef[1]*affordibility + intercept\n",
    "    return sigmoid(weighted_sum)\n",
    "\n",
    "prediction_function(.47, 1)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "prediction_function(.18, 1)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Now we start implementing gradient descent in plain python. Again the goal is to come up with same w1, w2 and bias that keras model calculated. We want to show how keras/tensorflow would have computed these values internally using gradient descent**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**First write couple of helper routines such as sigmoid and log_loss**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sigmoid_numpy(X):\n",
    "   return 1/(1+np.exp(-X))\n",
    "\n",
    "sigmoid_numpy(np.array([12,0,1]))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def log_loss(y_true, y_predicted):\n",
    "    epsilon = 1e-15\n",
    "    y_predicted_new = [max(i,epsilon) for i in y_predicted]\n",
    "    y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]\n",
    "    y_predicted_new = np.array(y_predicted_new)\n",
    "    return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**All right now comes the time to implement our final gradient descent function !! yay !!!**"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T16:51:11.341733Z",
     "start_time": "2026-01-24T16:51:11.327208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class myNN:\n",
    "    def __init__(self):\n",
    "        self.w1 = 1\n",
    "        self.w2 = 1\n",
    "        self.bias = 0\n",
    "    def fit(self, X, y, epochs,loss_thresold):\n",
    "        self.w1 , self.w2 , self.bias = self.gradient_descent(X['age'],X['affordibility'],y, epochs, loss_thresold)\n",
    "        print(f'final weights and bias : w1 : {self.w1} w2 : {self.w2} bias : {self.bias}')\n",
    "    def predict(self,X_test):\n",
    "        weighted_sum = self.w1*X_test['age'] + self.w2*X_test['affordibility'] + self.bias\n",
    "        return sigmoid_numpy(weighted_sum)\n",
    "    def gradient_descent(self,age, affordability, y_true, epochs, loss_thresold):\n",
    "        w1 = w2 = 1\n",
    "        bias = 0\n",
    "        rate = 0.5\n",
    "        n = len(age)\n",
    "        for i in range(epochs):\n",
    "            weighted_sum = w1 * age + w2 * affordability + bias\n",
    "            y_predicted = sigmoid_numpy(weighted_sum)\n",
    "            loss = log_loss(y_true, y_predicted)\n",
    "\n",
    "            w1d = (1/n)*np.dot(np.transpose(age),(y_predicted-y_true))\n",
    "            w2d = (1/n)*np.dot(np.transpose(affordability),(y_predicted-y_true))\n",
    "\n",
    "            bias_d = np.mean(y_predicted-y_true)\n",
    "            w1 = w1 - rate * w1d\n",
    "            w2 = w2 - rate * w2d\n",
    "            bias = bias - rate * bias_d\n",
    "\n",
    "            if i%50==0:\n",
    "                print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')\n",
    "\n",
    "            if loss<=loss_thresold:\n",
    "                print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')\n",
    "                break\n",
    "\n",
    "        return w1, w2, bias"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T16:51:16.496206Z",
     "start_time": "2026-01-24T16:51:16.249882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "customModel = myNN()\n",
    "customModel.fit(X_train_scaled, y_train, epochs=8000, loss_thresold=0.4631)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, w1:0.974907633470177, w2:0.948348125394529, bias:-0.11341867736368583, loss:0.7113403233723417\n",
      "Epoch:50, w1:1.503319554173139, w2:1.108384790367645, bias:-1.2319047301235464, loss:0.5675865113475955\n",
      "Epoch:100, w1:2.200713131760032, w2:1.2941584023238903, bias:-1.6607009122062801, loss:0.5390680417774752\n",
      "Epoch:150, w1:2.8495727769689085, w2:1.3696895491572745, bias:-1.986105845859897, loss:0.5176462164249294\n",
      "Epoch:200, w1:3.443016970881803, w2:1.4042218624465033, bias:-2.2571369883752723, loss:0.5005011269691375\n",
      "Epoch:250, w1:3.982450494649576, w2:1.4239127329321233, bias:-2.494377365971801, loss:0.48654089537617085\n",
      "Epoch:300, w1:4.472179522095915, w2:1.438787986553552, bias:-2.707387811922373, loss:0.4750814640632793\n",
      "Epoch:350, w1:4.917245868007634, w2:1.4525660781176122, bias:-2.901176333556766, loss:0.46561475306999006\n",
      "Epoch:366, w1:5.051047623653049, w2:1.4569794548473887, bias:-2.9596534546250037, loss:0.46293944095888917\n",
      "final weights and bias : w1 : 5.051047623653049 w2 : 1.4569794548473887 bias : -2.9596534546250037\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T16:51:50.857955Z",
     "start_time": "2026-01-24T16:51:50.846950Z"
    }
   },
   "cell_type": "code",
   "source": "coef, intercept",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.0608625],\n",
       "        [1.408652 ]], dtype=float32),\n",
       " array([-2.9137027], dtype=float32))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**This shows that in the end we were able to come up with same value of w1,w2 and bias using a plain python implementation of gradient descent function**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
